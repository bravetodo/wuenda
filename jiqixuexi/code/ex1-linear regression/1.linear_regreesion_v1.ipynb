{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regreesion（线性回归）\n",
    "注意：python版本为3.6，\n",
    "安装TensorFlow的方法：pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\", style=\"whitegrid\", palette=\"dark\")\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ex1data1.txt', names=['population', 'profit'])#读取数据并赋予列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()#看前五行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 看下原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('population', 'profit', df, size=6, fit_reg=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X(df):#读取特征\n",
    "#     \"\"\"\n",
    "#     use concat to add intersect feature to avoid side effect\n",
    "#     not efficient for big dataset though\n",
    "#     \"\"\"\n",
    "    ones = pd.DataFrame({'ones': np.ones(len(df))})#ones是m行1列的dataframe\n",
    "    data = pd.concat([ones, df], axis=1)  # 合并数据，根据列合并\n",
    "    return data.iloc[:, :-1].as_matrix()  # 这个操作返回 ndarray,不是矩阵\n",
    "\n",
    "\n",
    "def get_y(df):#读取标签\n",
    "#     '''assume the last column is the target'''\n",
    "    return np.array(df.iloc[:, -1])#df.iloc[:, -1]是指df的最后一列\n",
    "\n",
    "\n",
    "def normalize_feature(df):\n",
    "#     \"\"\"Applies function along input axis(default 0) of DataFrame.\"\"\"\n",
    "    return df.apply(lambda column: (column - column.mean()) / column.std())#特征缩放"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多变量的假设 h 表示为：\\\\[{{h}_{\\theta }}\\left( x \\right)={{\\theta }_{0}}+{{\\theta }_{1}}{{x}_{1}}+{{\\theta }_{2}}{{x}_{2}}+...+{{\\theta }_{n}}{{x}_{n}}\\\\] \n",
    "这个公式中有n+1个参数和n个变量，为了使得公式能够简化一些，引入${{x}_{0}}=1$，则公式转化为：  \n",
    "此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是 m*(n+1)。 因此公式可以简化为：${{h}_{\\theta }}\\left( x \\right)={{\\theta }^{T}}X$，其中上标T代表矩阵转置。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X_data, y_data, alpha, epoch, optimizer=tf.train.GradientDescentOptimizer):# 这个函数是旧金山的一个大神Lucas Shen写的\n",
    "      # placeholder for graph input\n",
    "    X = tf.placeholder(tf.float32, shape=X_data.shape)\n",
    "    y = tf.placeholder(tf.float32, shape=y_data.shape)\n",
    "\n",
    "    # construct the graph\n",
    "    with tf.variable_scope('linear-regression'):\n",
    "        W = tf.get_variable(\"weights\",\n",
    "                            (X_data.shape[1], 1),\n",
    "                            initializer=tf.constant_initializer())  # n*1\n",
    "\n",
    "        y_pred = tf.matmul(X, W)  # m*n @ n*1 -> m*1\n",
    "\n",
    "        loss = 1 / (2 * len(X_data)) * tf.matmul((y_pred - y), (y_pred - y), transpose_a=True)  # (m*1).T @ m*1 = 1*1\n",
    "\n",
    "    opt = optimizer(learning_rate=alpha)\n",
    "    opt_operation = opt.minimize(loss)\n",
    "\n",
    "    # run the session\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        loss_data = []\n",
    "\n",
    "        for i in range(epoch):\n",
    "            _, loss_val, W_val = sess.run([opt_operation, loss, W], feed_dict={X: X_data, y: y_data})\n",
    "            loss_data.append(loss_val[0, 0])  # because every loss_val is 1*1 ndarray\n",
    "\n",
    "            if len(loss_data) > 1 and np.abs(loss_data[-1] - loss_data[-2]) < 10 ** -9:  # early break when it's converged\n",
    "                # print('Converged at epoch {}'.format(i))\n",
    "                break\n",
    "\n",
    "    # clear the graph\n",
    "    tf.reset_default_graph()\n",
    "    return {'loss': loss_data, 'parameters': W_val}  # just want to return in row vector format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X_data, y_data, alpha, epoch):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ex1data1.txt', names=['population', 'profit'])#读取数据，并赋予列名\n",
    "\n",
    "data.head()#看下数据前5行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算代价函数\n",
    "$$J\\left( \\theta  \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( {{h}_{\\theta }}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}} \\right)}^{2}}}$$\n",
    "其中：\\\\[{{h}_{\\theta }}\\left( x \\right)={{\\theta }^{T}}X={{\\theta }_{0}}{{x}_{0}}+{{\\theta }_{1}}{{x}_{1}}+{{\\theta }_{2}}{{x}_{2}}+...+{{\\theta }_{n}}{{x}_{n}}\\\\] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_X(data)\n",
    "print(X.shape, type(X))\n",
    "\n",
    "y = get_y(data)\n",
    "print(y.shape, type(y))\n",
    "#看下数据维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.zeros(X.shape[1])#X.shape[1]=2,代表特征数n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_cost(theta, X, y):\n",
    "#     \"\"\"\n",
    "#     X: R(m*n), m 样本数, n 特征数\n",
    "#     y: R(m)\n",
    "#     theta : R(n), 线性回归的参数\n",
    "#     \"\"\"\n",
    "    m = X.shape[0]#m为样本数\n",
    "\n",
    "    inner = X @ theta - y  # R(m*1)，X @ theta等价于X.dot(theta)\n",
    "\n",
    "    # 1*m @ m*1 = 1*1 in matrix multiplication\n",
    "    # but you know numpy didn't do transpose in 1d array, so here is just a\n",
    "    # vector inner product to itselves\n",
    "    square_sum = inner.T @ inner\n",
    "    cost = square_sum / (2 * m)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cost(theta, X, y)#返回theta的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch gradient decent（批量梯度下降）\n",
    "$${{\\theta }_{j}}:={{\\theta }_{j}}-\\alpha \\frac{\\partial }{\\partial {{\\theta }_{j}}}J\\left( \\theta  \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(theta, X, y):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    inner = X.T @ (X @ theta - y)  # (m,n).T @ (m, 1) -> (n, 1)，X @ theta等价于X.dot(theta)\n",
    "\n",
    "    return inner / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_decent(theta, X, y, epoch, alpha=0.01):\n",
    "#   拟合线性回归，返回参数和代价\n",
    "#     epoch: 批处理的轮数\n",
    "#     \"\"\"\n",
    "    cost_data = [lr_cost(theta, X, y)]\n",
    "    _theta = theta.copy()  # 拷贝一份，不和原来的theta混淆\n",
    "\n",
    "    for _ in range(epoch):\n",
    "        _theta = _theta - alpha * gradient(_theta, X, y)\n",
    "        cost_data.append(lr_cost(_theta, X, y))\n",
    "\n",
    "    return _theta, cost_data\n",
    "#批量梯度下降函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 500\n",
    "final_theta, cost_data = batch_gradient_decent(theta, X, y, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_theta\n",
    "#最终的theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_data\n",
    "# 看下代价数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算最终的代价\n",
    "lr_cost(final_theta, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize cost data（代价数据可视化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.tsplot(cost_data, time=np.arange(epoch+1))\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('cost')\n",
    "plt.show()\n",
    "#可以看到从第二轮代价数据变换很大，接下来平稳了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = final_theta[0] # intercept，Y轴上的截距\n",
    "m = final_theta[1] # slope，斜率\n",
    "\n",
    "plt.scatter(data.population, data.profit, label=\"Training data\")\n",
    "plt.plot(data.population, data.population*m + b, label=\"Prediction\")\n",
    "plt.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- 选修章节"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('ex1data2.txt', names=['square', 'bedrooms', 'price'])\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 标准化数据\n",
    "最简单的方法是令：\n",
    "\n",
    " \n",
    "\n",
    "其中  是平均值，sn 是标准差。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_feature(df):\n",
    "#     \"\"\"Applies function along input axis(default 0) of DataFrame.\"\"\"\n",
    "    return df.apply(lambda column: (column - column.mean()) / column.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = normalize_feature(raw_data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. multi-var batch gradient decent（多变量批量梯度下降）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_X(data)\n",
    "print(X.shape, type(X))\n",
    "\n",
    "y = get_y(data)\n",
    "print(y.shape, type(y))#看下数据的维度和类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01#学习率\n",
    "theta = np.zeros(X.shape[1])#X.shape[1]：特征数n\n",
    "epoch = 500#轮数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_theta, cost_data = batch_gradient_decent(theta, X, y, epoch, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.tsplot(time=np.arange(len(cost_data)), data = cost_data)\n",
    "plt.xlabel('epoch', fontsize=18)\n",
    "plt.ylabel('cost', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. learning rate（学习率）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = np.logspace(-1, -5, num=4)\n",
    "candidate = np.sort(np.concatenate((base, base*3)))\n",
    "print(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=50\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "for alpha in candidate:\n",
    "    _, cost_data = batch_gradient_decent(theta, X, y, epoch, alpha=alpha)\n",
    "    ax.plot(np.arange(epoch+1), cost_data, label=alpha)\n",
    "\n",
    "ax.set_xlabel('epoch', fontsize=18)\n",
    "ax.set_ylabel('cost', fontsize=18)\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "ax.set_title('learning rate', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. normal equation（正规方程）\n",
    "正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\\frac{\\partial }{\\partial {{\\theta }_{j}}}J\\left( {{\\theta }_{j}} \\right)=0$ 。\n",
    " 假设我们的训练集特征矩阵为 X（包含了${{x}_{0}}=1$）并且我们的训练集结果为向量 y，则利用正规方程解出向量 $\\theta ={{\\left( {{X}^{T}}X \\right)}^{-1}}{{X}^{T}}y$ 。\n",
    "上标T代表矩阵转置，上标-1 代表矩阵的逆。设矩阵$A={{X}^{T}}X$，则：${{\\left( {{X}^{T}}X \\right)}^{-1}}={{A}^{-1}}$\n",
    "\n",
    "梯度下降与正规方程的比较：\n",
    "\n",
    "梯度下降：需要选择学习率α，需要多次迭代，当特征数量n大时也能较好适用，适用于各种类型的模型\t\n",
    "\n",
    "正规方程：不需要选择学习率α，一次计算得出，需要计算${{\\left( {{X}^{T}}X \\right)}^{-1}}$，如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为O(n3)，通常来说当n小于10000 时还是可以接受的，只适用于线性模型，不适合逻辑回归模型等其他模型\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正规方程\n",
    "def normalEqn(X, y):\n",
    "    theta = np.linalg.inv(X.T@X)@X.T@y#X.T@X等价于X.T.dot(X)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_theta2=normalEqn(X, y)#感觉和批量梯度下降的theta的值有点差距\n",
    "final_theta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the tensorflow graph over several optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = get_X(data)\n",
    "print(X_data.shape, type(X_data))\n",
    "\n",
    "y_data = get_y(data).reshape(len(X_data), 1)  # special treatment for tensorflow input data\n",
    "print(y_data.shape, type(y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2000\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_dict={'GD': tf.train.GradientDescentOptimizer,\n",
    "                'Adagrad': tf.train.AdagradOptimizer,\n",
    "                'Adam': tf.train.AdamOptimizer,\n",
    "                'Ftrl': tf.train.FtrlOptimizer,\n",
    "                'RMS': tf.train.RMSPropOptimizer\n",
    "               }\n",
    "results = []\n",
    "for name in optimizer_dict:\n",
    "    res = linear_regression(X_data, y_data, alpha, epoch, optimizer=optimizer_dict[name])\n",
    "    res['name'] = name\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "for res in results: \n",
    "    loss_data = res['loss']\n",
    "    \n",
    "#     print('for optimizer {}'.format(res['name']))\n",
    "#     print('final parameters\\n', res['parameters'])\n",
    "#     print('final loss={}\\n'.format(loss_data[-1]))\n",
    "    ax.plot(np.arange(len(loss_data)), loss_data, label=res['name'])\n",
    "\n",
    "ax.set_xlabel('epoch', fontsize=18)\n",
    "ax.set_ylabel('cost', fontsize=18)\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "ax.set_title('different optimizer', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
